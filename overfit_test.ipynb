{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3596d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4I import benchmarks, clustering, evaluation, investment, risk_factors2, utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from numpy import corrcoef\n",
    "from pandas import DataFrame\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1229cd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1955, 151)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Dataset'\n",
    "X = pd.read_csv(f'{path}/Stock_Market.csv', index_col=0)\n",
    "X.index = pd.to_datetime(X.index)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29245730",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pre = \"MinMax\"\n",
    "X.index = pd.to_datetime(X.index)\n",
    "\n",
    "# divide train test dataset\n",
    "\n",
    "train_ratio = 0.8\n",
    "split_index = int(len(X) * train_ratio)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "\n",
    "# Monthly returns\n",
    "XM = utils.DailyPrice2MonthlyReturn(X_train)\n",
    "# Daily returns\n",
    "XD = utils.DailyPrice2DailyReturn(X_train)\n",
    "# Scale data\n",
    "if scaler_pre == 'Standard':\n",
    "    XD = pd.DataFrame( StandardScaler().fit_transform(XD.values),\n",
    "                      index = XD.index, columns = XD.columns )\n",
    "    XM = pd.DataFrame( StandardScaler().fit_transform(XM.values),\n",
    "                      index = XM.index, columns = XM.columns )\n",
    "elif scaler_pre == 'MinMax':\n",
    "    XD = pd.DataFrame( MinMaxScaler().fit_transform(XD.values),\n",
    "                      index = XD.index, columns = XD.columns )\n",
    "    XM = pd.DataFrame( MinMaxScaler().fit_transform(XM.values),\n",
    "                      index = XM.index, columns = XM.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92c4cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "XM2 = utils.DailyPrice2MonthlyReturn(X_test)\n",
    "# Daily returns\n",
    "XD2 = utils.DailyPrice2DailyReturn(X_test)\n",
    "XD2 = pd.DataFrame( MinMaxScaler().fit_transform(XD2.values),\n",
    "                  index = XD2.index, columns = XD2.columns )\n",
    "XM2 = pd.DataFrame( MinMaxScaler().fit_transform(XM2.values),\n",
    "                  index = XM2.index, columns = XM2.columns )\n",
    "dfs2 = [XD2, XM2]\n",
    "gran_names=['Daily','Monthly']\n",
    "threshold=0.5\n",
    "SEED=1\n",
    "n_pcs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0590864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57590a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 151)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XD2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "27b49aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - 1s 956us/step - loss: 0.2738\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 0s 968us/step - loss: 0.2479\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 0s 997us/step - loss: 0.2386\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.2306\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 0s 910us/step - loss: 0.2235\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 0s 902us/step - loss: 0.2171\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 0s 959us/step - loss: 0.2108\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.2045\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 0s 992us/step - loss: 0.1985\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.1928\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.1872\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 0s 975us/step - loss: 0.1816\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 0s 966us/step - loss: 0.1761\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 0s 973us/step - loss: 0.1707\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 0s 921us/step - loss: 0.1657\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 0s 847us/step - loss: 0.1605\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 0s 938us/step - loss: 0.1557\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 0s 921us/step - loss: 0.1506\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 0s 866us/step - loss: 0.1459\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 0s 886us/step - loss: 0.1411\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 0s 898us/step - loss: 0.1364\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 0s 869us/step - loss: 0.1322\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 0s 862us/step - loss: 0.1274\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - 0s 951us/step - loss: 0.1234\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - 0s 988us/step - loss: 0.1192\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.1148\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.1113\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.1075\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - 0s 993us/step - loss: 0.1032\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - 0s 910us/step - loss: 0.1002\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - 0s 908us/step - loss: 0.0966\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - 0s 935us/step - loss: 0.0928\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - 0s 874us/step - loss: 0.0902\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - 0s 833us/step - loss: 0.0869\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - 0s 891us/step - loss: 0.0837\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - 0s 881us/step - loss: 0.0809\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0781\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - 0s 905us/step - loss: 0.0757\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.0731\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0718\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0681\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0663\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - 0s 912us/step - loss: 0.0648\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - 0s 842us/step - loss: 0.0623\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - 0s 827us/step - loss: 0.0606\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - 0s 957us/step - loss: 0.0584\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - 0s 890us/step - loss: 0.0569\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - 0s 830us/step - loss: 0.0555\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - 0s 843us/step - loss: 0.0550\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - 0s 889us/step - loss: 0.0531\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - 0s 834us/step - loss: 0.0516\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - 0s 978us/step - loss: 0.0502\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.0493\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - 0s 945us/step - loss: 0.0479\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.0473\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.0464\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - 0s 846us/step - loss: 0.0451\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - 0s 982us/step - loss: 0.0444\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0437\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - 0s 999us/step - loss: 0.0425\n",
      "Epoch 61/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.0422\n",
      "Epoch 62/100\n",
      "98/98 [==============================] - 0s 999us/step - loss: 0.0418\n",
      "Epoch 63/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.0406\n",
      "Epoch 64/100\n",
      "98/98 [==============================] - 0s 848us/step - loss: 0.0402\n",
      "Epoch 65/100\n",
      "98/98 [==============================] - 0s 800us/step - loss: 0.0392\n",
      "Epoch 66/100\n",
      "98/98 [==============================] - 0s 961us/step - loss: 0.0387\n",
      "Epoch 67/100\n",
      "98/98 [==============================] - 0s 997us/step - loss: 0.0387\n",
      "Epoch 68/100\n",
      "98/98 [==============================] - 0s 999us/step - loss: 0.0381\n",
      "Epoch 69/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0371\n",
      "Epoch 70/100\n",
      "98/98 [==============================] - 0s 911us/step - loss: 0.0371\n",
      "Epoch 71/100\n",
      "98/98 [==============================] - 0s 961us/step - loss: 0.0362\n",
      "Epoch 72/100\n",
      "98/98 [==============================] - 0s 878us/step - loss: 0.0360\n",
      "Epoch 73/100\n",
      "98/98 [==============================] - 0s 945us/step - loss: 0.0358\n",
      "Epoch 74/100\n",
      "98/98 [==============================] - 0s 938us/step - loss: 0.0349\n",
      "Epoch 75/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0346\n",
      "Epoch 76/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0342\n",
      "Epoch 77/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.0339\n",
      "Epoch 78/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0341\n",
      "Epoch 79/100\n",
      "98/98 [==============================] - 0s 832us/step - loss: 0.0337\n",
      "Epoch 80/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.0327\n",
      "Epoch 81/100\n",
      "98/98 [==============================] - 0s 871us/step - loss: 0.0327\n",
      "Epoch 82/100\n",
      "98/98 [==============================] - 0s 882us/step - loss: 0.0324\n",
      "Epoch 83/100\n",
      "98/98 [==============================] - 0s 816us/step - loss: 0.0319\n",
      "Epoch 84/100\n",
      "98/98 [==============================] - 0s 822us/step - loss: 0.0323\n",
      "Epoch 85/100\n",
      "98/98 [==============================] - 0s 904us/step - loss: 0.0315\n",
      "Epoch 86/100\n",
      "98/98 [==============================] - 0s 864us/step - loss: 0.0316\n",
      "Epoch 87/100\n",
      "98/98 [==============================] - 0s 916us/step - loss: 0.0313\n",
      "Epoch 88/100\n",
      "98/98 [==============================] - 0s 999us/step - loss: 0.0303\n",
      "Epoch 89/100\n",
      "98/98 [==============================] - 0s 970us/step - loss: 0.0302\n",
      "Epoch 90/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 91/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.0305\n",
      "Epoch 92/100\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 0.0300\n",
      "Epoch 93/100\n",
      "98/98 [==============================] - 0s 1000us/step - loss: 0.0301\n",
      "Epoch 94/100\n",
      "98/98 [==============================] - 0s 998us/step - loss: 0.0295\n",
      "Epoch 95/100\n",
      "98/98 [==============================] - 0s 913us/step - loss: 0.0291\n",
      "Epoch 96/100\n",
      "98/98 [==============================] - 0s 855us/step - loss: 0.0295\n",
      "Epoch 97/100\n",
      "98/98 [==============================] - 0s 956us/step - loss: 0.0290\n",
      "Epoch 98/100\n",
      "98/98 [==============================] - 0s 901us/step - loss: 0.0282\n",
      "Epoch 99/100\n",
      "98/98 [==============================] - 0s 988us/step - loss: 0.0289\n",
      "Epoch 100/100\n",
      "98/98 [==============================] - 0s 965us/step - loss: 0.0283\n",
      "49/49 [==============================] - 0s 671us/step\n",
      "13/13 [==============================] - 0s 720us/step\n",
      "name: Daily\n",
      "train mse 0.010588732516561486\n",
      "test mse 0.02783256445649623\n"
     ]
    }
   ],
   "source": [
    "# nnpca on the daily data is easy to overfit, divide it into 2 models, on monthly is still the previous one\n",
    "dfs = [XD, XM] \n",
    "dfs2 = [XD2,XM2]\n",
    "gran_names=['Daily','Monthly']\n",
    "threshold=0.5\n",
    "SEED=1\n",
    "n_pcss = [5,7]\n",
    "\n",
    "Encoded = list()\n",
    "\n",
    "i = 0\n",
    "df = dfs[i]\n",
    "name = gran_names[i]\n",
    "n_pcs = n_pcss[i]\n",
    "input_dim = df.shape[1]\n",
    "\n",
    "# Define a deeper autoencoder architecture\n",
    "input_layer = keras.layers.Input(shape=(input_dim,))\n",
    "encoded_1 = keras.layers.Dense(16, activation='tanh', activity_regularizer=regularizers.l1(10**(-4)))(input_layer)\n",
    "encoded_1 = Dropout(0.5)(encoded_1)\n",
    "encoded_2 = keras.layers.Dense(8, activation='relu', activity_regularizer=regularizers.l1(10**(-4)))(encoded_1)\n",
    "encoded_2 = Dropout(0.5)(encoded_2)\n",
    "encoded_final = keras.layers.Dense(n_pcs, activation='relu', activity_regularizer=regularizers.l1(10**(-4)))(encoded_2)\n",
    "\n",
    "# Symmetric decoder architecture\n",
    "decoded_1 = keras.layers.Dense(8, activation='relu', activity_regularizer=regularizers.l1(10**(-4)))(encoded_final)\n",
    "decoded_1 = Dropout(0.5)(decoded_1)\n",
    "decoded_2 = keras.layers.Dense(16, activation='tanh', activity_regularizer=regularizers.l1(10**(-4)))(decoded_1)\n",
    "decoded_2 = Dropout(0.5)(decoded_2)\n",
    "decoded_final = keras.layers.Dense(input_dim, activation='linear', activity_regularizer=regularizers.l1(10**(-4)))(decoded_2)\n",
    "\n",
    "autoencoder = keras.Model(input_layer, decoded_final)\n",
    "encoder = keras.Model(input_layer, encoded_final)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "autoencoder.compile(optimizer='SGD', loss='mean_squared_error')\n",
    "autoencoder.fit(df.values, df.values, epochs=100, batch_size=16, shuffle=True,callbacks=[early_stopping])\n",
    "\n",
    "\"\"\"\n",
    "encoded_data = encoder.predict(dfs[0].values)\n",
    "Encoded.append(DataFrame(encoded_data,\n",
    "                         index=dfs[0].index,\n",
    "                         columns=[f'{name}_{k}th' for k in range(1, n_pcs+1)]))\n",
    "\"\"\"\n",
    "dfs_reconstructed = autoencoder.predict(dfs[i].values)\n",
    "mse_train = np.mean((dfs[i].values - dfs_reconstructed) ** 2)\n",
    "\n",
    "\n",
    "dfs2_reconstructed = autoencoder.predict(dfs2[i].values)\n",
    "mse_test = np.mean((dfs2[i].values - dfs2_reconstructed) ** 2)\n",
    "\n",
    "print(\"name:\",name)\n",
    "print(\"train mse\",mse_train)\n",
    "print(\"test mse\",mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d011491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1951\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0579\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0309\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0266\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0244\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0238\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0230\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0224\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0222\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0219\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0216\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0214\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0209\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0210\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 938us/step - loss: 0.0206\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0206\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 510us/step - loss: 0.0205\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 405us/step - loss: 0.0204\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0203\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0200\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 860us/step - loss: 0.0199\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0198\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0197\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0195\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0191\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0186\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0185\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0184\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 910us/step - loss: 0.0181\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 62us/step - loss: 0.0178\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0178\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 55us/step - loss: 0.0177\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0175\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.0170\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 640us/step - loss: 0.0169\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "name: Monthly\n",
      "train mse 0.016776364777522947\n",
      "test mse 0.0650307817350745\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "df = dfs[i]\n",
    "name = gran_names[i]\n",
    "n_pcs = n_pcss[i]\n",
    "input_dim = df.shape[1]\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(input_dim,))\n",
    "encoded_1 = keras.layers.Dense(128, activation='tanh')(input_layer)\n",
    "encoded_2 = keras.layers.Dense(64, activation='tanh')(encoded_1)\n",
    "encoded_final = keras.layers.Dense(n_pcs, activation='tanh')(encoded_2)\n",
    "\n",
    "# Symmetric decoder architecture\n",
    "decoded_1 = keras.layers.Dense(64, activation='tanh')(encoded_final)\n",
    "decoded_2 = keras.layers.Dense(128, activation='tanh')(decoded_1)\n",
    "decoded_final = keras.layers.Dense(input_dim, activation='linear')(decoded_2)\n",
    "\n",
    "autoencoder = keras.Model(input_layer, decoded_final)\n",
    "encoder = keras.Model(input_layer, encoded_final)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(df.values, df.values, epochs=100, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "encoder = keras.Model(input_layer, encoded_final)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "autoencoder.fit(df.values, df.values, epochs=100, batch_size=16, shuffle=True,callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "dfs_reconstructed = autoencoder.predict(dfs[i].values)\n",
    "mse_train = np.mean((dfs[i].values - dfs_reconstructed) ** 2)\n",
    "\n",
    "\n",
    "dfs2_reconstructed = autoencoder.predict(dfs2[i].values)\n",
    "mse_test = np.mean((dfs2[i].values - dfs2_reconstructed) ** 2)\n",
    "\n",
    "print(\"name:\",name)\n",
    "print(\"train mse\",mse_train)\n",
    "print(\"test mse\",mse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
